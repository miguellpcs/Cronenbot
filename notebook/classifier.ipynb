{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cronenberg Movie Assistent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_paths = []\n",
    "trainset_path = '../../dstc8-schema-guided-dialogue/train/'\n",
    "for file in os.listdir(trainset_path):\n",
    "    if file != 'schema.json':\n",
    "        with open(os.path.join(trainset_path, file), 'r') as json_file:\n",
    "            for example in json.load(json_file):\n",
    "                train_paths.append(example)\n",
    "\n",
    "\n",
    "test_paths = []\n",
    "testset_path = '../../dstc8-schema-guided-dialogue/test/'\n",
    "for file in os.listdir(testset_path):\n",
    "    if file != 'schema.json':\n",
    "        with open(os.path.join(testset_path, file), 'r') as json_file:\n",
    "            for example in json.load(json_file):\n",
    "                test_paths.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_sentences_0 = []\n",
    "train_sentences_1 = []\n",
    "\n",
    "others_train_sentences = []\n",
    "none_train_sentences = []\n",
    "\n",
    "intents = {'FindMovies':0, 'PlayMovie':0, 'NONE':0, 'others':0}\n",
    "for dialogue in train_paths:\n",
    "    for idx in range(0, len(dialogue['turns']), 2):\n",
    "        sentence = dialogue['turns'][idx]['utterance']\n",
    "        intent = dialogue['turns'][idx]['frames'][0]['state']['active_intent']\n",
    "\n",
    "        if intent not in ['FindMovies', 'PlayMovie', 'NONE']:\n",
    "          intents['others'] += 1\n",
    "          others_train_sentences.append(sentence)\n",
    "          pass\n",
    "        else:\n",
    "          if intent == 'FindMovies':\n",
    "            intents['FindMovies'] += 1\n",
    "            train_sentences_0.append(sentence)\n",
    "          elif intent == 'PlayMovie':\n",
    "            intents['PlayMovie'] += 1\n",
    "            train_sentences_1.append(sentence)\n",
    "          else:\n",
    "            none_train_sentences.append(sentence)\n",
    "            intents['NONE'] += 1\n",
    "#print(intents)\n",
    "#print(len(train_sentences_0),len(train_sentences_1), len(none_train_sentences),len(others_train_sentences))\n",
    "#plt.bar(intents.keys(), intents.values(), 1.0, color='g')\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FindMovies': 6179, 'PlayMovie': 2579, 'NONE': 11971, 'others': 144253}\n",
      "1744 824 2911 36818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_intents = []\n",
    "test_sentences_0 = []\n",
    "test_sentences_1 = []\n",
    "\n",
    "others_test_sentences = []\n",
    "none_test_sentences = []\n",
    "for dialogue in test_paths:\n",
    "\n",
    "    for idx in range(0, len(dialogue['turns']), 2):\n",
    "        sentence = dialogue['turns'][idx]['utterance']\n",
    "        intent = dialogue['turns'][idx]['frames'][0]['state']['active_intent']\n",
    "\n",
    "        if intent not in ['FindMovies', 'PlayMovie', 'NONE']:\n",
    "          others_test_sentences.append(sentence)\n",
    "          pass\n",
    "        else:\n",
    "          if intent == 'FindMovies':\n",
    "        \n",
    "            test_sentences_0.append(sentence)\n",
    "          elif intent == 'PlayMovie':\n",
    "        \n",
    "            test_sentences_1.append(sentence)\n",
    "          else:\n",
    "            none_test_sentences.append(sentence)\n",
    "            \n",
    "print(intents)\n",
    "print(len(test_sentences_0),len(test_sentences_1), len(none_test_sentences),len(others_test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balacing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2579\n",
      "12895 12895\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([2579.,    0.,    0., 2579.,    0.,    0., 2579.,    0.,    0.,\n        5158.]),\n array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n <a list of 10 Patch objects>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_val = min(intents.values())\n",
    "print(min_val)\n",
    "\n",
    "input_train = []\n",
    "target_train = []\n",
    "\n",
    "\n",
    "input_train.extend(train_sentences_0[:min_val])\n",
    "target_train.extend([\"find_movie\" for _ in range(min_val)])\n",
    "\n",
    "\n",
    "input_train.extend(train_sentences_1[:min_val])\n",
    "target_train.extend([\"play_movie\" for _ in range(min_val)])\n",
    "\n",
    "\n",
    "\n",
    "input_train.extend(none_train_sentences[:min_val])\n",
    "target_train.extend([\"end_chat\" for _ in range(min_val)])\n",
    "\n",
    "input_train.extend(others_train_sentences[:2*min_val])\n",
    "target_train.extend([\"others\" for _ in range(2*min_val)])\n",
    "\n",
    "\n",
    "print(len(input_train), len(target_train))\n",
    "#print(input_train[12672], target_train[12672])\n",
    "plt.hist(target_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7951 7951\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_test = []\n",
    "target_test = []\n",
    "\n",
    "\n",
    "input_test.extend(test_sentences_0)\n",
    "target_test.extend([\"find_movie\" for _ in range(len(test_sentences_0))])\n",
    "\n",
    "input_test.extend(test_sentences_1)\n",
    "target_test.extend([\"play_movie\" for _ in range(len(test_sentences_1))])\n",
    "\n",
    "input_test.extend(none_test_sentences)\n",
    "target_test.extend([\"end_chat\" for _ in range(len(none_test_sentences))])\n",
    "\n",
    "\n",
    "input_test.extend(others_test_sentences[:3*len(test_sentences_1)])\n",
    "target_test.extend([\"others\" for _ in range(3*len(test_sentences_1))])\n",
    "\n",
    "print(len(input_test),len(target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_num_words = 40000\n",
    "classes = ['find_movie',\"play_movie\",\"end_chat\",\"others\"]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_num_words)\n",
    "tokenizer.fit_on_texts(input_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for c in input_train:\n",
    "    ls.append(len(c.split()))\n",
    "maxLen=int(np.percentile(ls, 98))\n",
    "train_sequences = tokenizer.texts_to_sequences(input_train)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=maxLen,              padding='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(input_test)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=maxLen, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(classes)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoder.fit(integer_encoded)\n",
    "train_label_encoded = label_encoder.transform(target_train)\n",
    "train_label_encoded = train_label_encoded.reshape(len(train_label_encoded), 1)\n",
    "train_label = onehot_encoder.transform(train_label_encoded)\n",
    "\n",
    "\n",
    "test_labels_encoded = label_encoder.transform(target_test)\n",
    "test_labels_encoded = test_labels_encoded.reshape(len(test_labels_encoded), 1)\n",
    "test_labels = onehot_encoder.transform(test_labels_encoded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'glove.6B.100d.txt'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "url ='https://www.dropbox.com/s/a247ju2qsczh0be/glove.6B.100d.txt?dl=1'\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index={}\n",
    "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dclm/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3263: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "num_words = min(max_num_words, len(word_index))+1\n",
    "embedding_dim=len(embeddings_index['the'])\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_num_words:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional,Embedding\n",
    "from tensorflow.nn import leaky_relu\n",
    "classes = np.array(classes)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, 100, trainable=False,input_length=train_sequences.shape[1], weights=[embedding_matrix]))\n",
    "model.add(Bidirectional(LSTM(512, return_sequences=True, recurrent_dropout=0.1, dropout=0.1), 'concat'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation=leaky_relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(classes.shape[0], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 27s 136ms/step - loss: 0.9163 - acc: 0.6261 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_sequences, train_label, epochs = 1,\n",
    "          batch_size = 64, shuffle=True, validation_data=[test_sequences,test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "model.save('models/intents.h5')\n",
    "\n",
    "with open('utils/classes.pkl','wb') as file:\n",
    "   pickle.dump(classes,file)\n",
    "\n",
    "with open('utils/tokenizer.pkl','wb') as file:\n",
    "   pickle.dump(tokenizer,file)\n",
    "\n",
    "with open('utils/label_encoder.pkl','wb') as file:\n",
    "   pickle.dump(label_encoder,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "class IntentClassifier:\n",
    "    def __init__(self,classes,model,tokenizer,label_encoder):\n",
    "        self.classes = classes\n",
    "        self.classifier = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def get_intent(self,text):\n",
    "        self.text = [text]\n",
    "        self.test_keras = self.tokenizer.texts_to_sequences(self.text)\n",
    "        self.test_keras_sequence = pad_sequences(self.test_keras, maxlen=16, padding='post')\n",
    "        self.pred = self.classifier.predict(self.test_keras_sequence)\n",
    "        return self.label_encoder.inverse_transform(np.argmax(self.pred,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from tensorflow.python.keras.models import load_model\n",
    "import tensorflow.nn as nn\n",
    "model = load_model('models/intents_new.h5',custom_objects={'leaky_relu': nn.leaky_relu})\n",
    "\n",
    "with open('utils/classes_new.pkl','rb') as file:\n",
    "  classes = pickle.load(file)\n",
    "\n",
    "with open('utils/tokenizer_new.pkl','rb') as file:\n",
    "  tokenizer = pickle.load(file)\n",
    "\n",
    "with open('utils/label_encoder_new.pkl','rb') as file:\n",
    "  label_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 21) for input Tensor(\"embedding_13_input:0\", shape=(None, 21), dtype=float32), but it was called on an input with incompatible shape (None, 16).\n",
      "Phrase: I wanna watch a horror movie\n",
      "Predicted intent: find_movie\n"
     ]
    }
   ],
   "source": [
    "def decode(label):\n",
    "    if label == 0:\n",
    "        return \"find_movie\"\n",
    "    if label == 1: \n",
    "        return \"play_movie\"\n",
    "    if label == 2:\n",
    "        return \"end_chat\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "phrase = \"I wanna watch a horror movie\"\n",
    "nlu = IntentClassifier(classes,model,tokenizer,label_encoder)\n",
    "label = nlu.get_intent(phrase)\n",
    "print(\"Phrase: {}\".format(phrase))\n",
    "print(\"Predicted intent: {}\".format((label)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}